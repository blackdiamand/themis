---
import Base from "@layouts/base.astro";
import ReportCard from "@components/index-report-card.astro";
import QuestionCard from "@components/question-card-detailed.astro";
import SeeMoreLink from "@components/see-more-link.astro";
import PlatformCard from "@components/platform-card.astro";
import CalibrationDefault from "@components/charts/calibration-default.astro";
import RelScoreExplainer from "@components/charts/index-relscore-explainer.astro";
import type { CategoryDetails, MarketDetails, PlatformDetails } from "@types";
import {
  getMarkets,
  getPlatforms,
  getCategories,
  getPlatformCategoryScores,
  getFeaturedQuestions,
  getPlatformOverallScores,
  getQuestionCount,
} from "@lib/api";

let scoreType = "brier-relative";

const platforms: PlatformDetails[] = await getPlatforms();
const categories: CategoryDetails[] = await getCategories();
const markets: MarketDetails[] = await getMarkets();
const platformCategoryScores = await getPlatformCategoryScores(scoreType);
const platformOverallScores = await getPlatformOverallScores();
const featuredQuestions = await getFeaturedQuestions(4);
const { numQuestions, numLinkedMarkets } = await getQuestionCount();
---

<Base title="Home">
  <div class="grid grid-cols-1 md:grid-cols-2 pt-4">
    <div class="p-4 m-2">
      <h1 class="text-4xl font-bold">How accurate are prediction markets?</h1>
      <p class="my-2">
        In the past it's been hard to measure the accuracy of prediction markets
        since question difficulty can vary so drastically. You can't compare
        apples to oranges!
      </p>
      <p class="my-2">
        In order to solve that problem we've matched markets across platforms
        and curated a collection of predictive questions that are actually
        comparable. Using those matches, we can generate a score for each
        platform in each category that rewards whichever market was correct
        earliest.
      </p>
      <ul class="list-disc px-4">
        <li>
          <a href="#intro" class="underline"> What's a prediction market? </a>
        </li>
        <li>
          <a href="#scoring" class="underline">
            How do you calculate the scores?
          </a>
        </li>
        <li>
          <a href="#calibration" class="underline"> What about calibration? </a>
        </li>
      </ul>
    </div>
    <div class="">
      <ReportCard
        platforms={platforms}
        categories={categories}
        platformCategoryScores={platformCategoryScores}
        platformOverallScores={platformOverallScores}
      />
    </div>
  </div>
  <hr class="my-2" />
  <div class="p-4 m-2">
    <h1 class="text-4xl font-bold">Top Questions</h1>
    <p class="mt-2">
      Each card here asks a general question that several prediction market
      platforms have indepenently tried to predict. We find and link markets
      when they resolve in order to judge all platforms on an even playing
      field. As of {new Date().toLocaleDateString()} we have {
        numLinkedMarkets.toLocaleString()
      } linked markets across
      {numQuestions.toLocaleString()} unique questions.
    </p>
    <p class="mt-2">
      A traditional accuracy analysis would look at a single point in time,
      usually midway through the market. We calculate our absolute scores in
      this way, and show it below as the midpoint Brier score.
    </p>
    <p class="mt-2">
      However, that form of scoring often misses a lot of important information.
      When you look at the probability charts below, you can see how each
      market's prediction changes over time as they respond to news, polls, and
      other information. In addition, each prediction platform has different
      rules on what they predict and how early their markets open which makes a
      direct comparison difficult.
    </p>
    <p class="mt-2">
      In order to address this, we start by scoring every market on every day
      that it's open. Then we aggregate them into a relative score - grading
      that market's performance relative to the other linked markets on each day
      and rewarding those that were correct earliest. Check out the <a
        href="#scoring"
        class="text-lavender">scoring section</a
      > for details about the system.
    </p>
  </div>
  <div>
    {
      featuredQuestions.map((question) => {
        return <QuestionCard question={question} categories={categories} />;
      })
    }
  </div>
  <SeeMoreLink link={`/questions`} text="See all questions" />
  <hr class="my-2" />
  <div class="p-4 m-2" id="intro">
    <h1 class="text-4xl font-bold">What's a prediction market?</h1>
    <p class="my-2">
      Predicting the future is hard, but it's also incredibly important. Let's
      say someone starts making predictions about important events. How much
      should you believe them when they say the world will end tomorrow? What
      about when they say there's a 70% chance the world will end in 50 years?
    </p>
    <p class="my-2">
      Prediction markets are based on a simple concept: If you're confident
      about something, you can place a bet on it. If someone else disagrees with
      you, declare terms with them and whoever wins takes the money. By
      aggregating the implied odds of these trades, you can gain an insight into
      the <b>wisdom of the crowds</b>.
    </p>
    <p class="my-2">
      Imagine a stock exchange, but instead of trading shares, you trade on the
      likelihood of future events. Each prediction market offers contracts tied
      to specific events, like elections, economic indicators, or scientific
      breakthroughs. You can buy or sell these contracts based on your belief
      about the outcome - if you are very confident about something, or you have
      specialized information, you can make a lot of money from a market.
    </p>
    <p class="my-2">
      Markets give participants a <b>financial incentive</b> to be correct, encouraging
      researchers and skilled forecasters to spend time investigating events. Individuals
      with insider information or niche skills can profit by trading, which also
      updates the market's probability. Prediction markets have <a
        href="https://daily.jstor.org/how-accurate-are-prediction-markets/"
        class="text-lavender"
      >
        out-performed polls
      </a> and <a
        href="https://news.manifold.markets/p/manifold-predicted-the-ai-extinction"
        class="text-lavender"
      >
        revealed insider information</a
      >, making them a useful tool for information gathering or profit.
    </p>
    <p class="my-2">Some popular prediction market platforms include:</p>
    <div class="grid grid-cols-1 md:grid-cols-2 lg:grid-cols-4">
      {platforms.map((platform) => <PlatformCard platform={platform} />)}
    </div>
    <SeeMoreLink link={`/platforms`} text="More information about platforms" />
  </div>
  <hr class="my-2" />
  <div class="p-4 m-2" id="scoring">
    <h1 class="text-4xl font-bold">How do you calculate the scores?</h1>
    <p class="my-2">
      The traditional way to score predictions is using Brier scores, which
      measure how far off your prediction was from reality. While these work
      great for individual predictions, they struggle to compare predictions
      across different time periods - being 90% confident a month before an
      event is more impressive than being 90% confident the day before.
    </p>
    <p class="my-2">
      To account for this, we use a relative Brier scoring system. For each
      matched question across platforms, we compare how early each platform
      reached the correct probability range. Platforms that arrive at accurate
      predictions earlier receive more points, while those that take longer or
      never reach accuracy receive fewer points.
    </p>
    <div class="grid gap-2 grid-cols-1 md:grid-cols-2">
      <div>
        <p class="my-2">
          As an example, let's look at the probability history for an actual set
          of markets.
        </p>
        <ul class="list-disc pl-5 space-y-1 mb-5">
          <li>
            Our first step is preprocessing - for every market we average the
            probability over each day to minimize transient spikes and normalize
            the data.
          </li>
          <li>
            Next we narrow the range down to the period where at least two
            markets are open. For some markets we will also override the start
            or end dates to limit the scoring period.
          </li>
          <li>
            For each day in the scoring range, we calculate the score for each
            market. In this case we will use the Brier score, but other scores
            such as log would also work. This is the daily absolute score.
          </li>
          <li>
            We then calculate the median daily score as a baseline for
            comparison. Markets that do better than this median will have better
            scores at the end.
          </li>
          <li>
            For each day, we find the difference between each market's daily
            absolute score and that median.
          </li>
          <li>
            Finally, we sum all of the market's daily differences and divide
            them by the total number of scoring days. Not all markets are open
            for the same duration, so this grants better scores to the markets
            that were open for longer.
          </li>
          <li>
            This gives us a number that can be graded similarly to a Brier
            score, in the sense that lower is better. However, the scores can
            now be from -1 to +1 and most will be centered around 0, the median
            score.
          </li>
          <li>
            In order to more easily evaluate these scores, we can assign then
            letter grades at certain cutoffs. These are the grades that you see
            on each question card.
          </li>
        </ul>
        <p class="my-2">
          We calculate these scores for all linked markets, since we are
          confident that they meet our standards for serious markets making real
          predictions. We can average these scores together to get overall
          scores for each platform, category, and combination therof.
        </p>
      </div>
      <div>
        <RelScoreExplainer question={featuredQuestions[0]} />
      </div>
    </div>
    <SeeMoreLink link={`/about#resources`} text="Sources & further reading" />
  </div>
  <hr class="my-2" />
  <div class="p-4 m-2" id="calibration">
    <h1 class="text-4xl font-bold">What about calibration?</h1>
    <p class="my-2">
      Accuracy is a good metric, but another lens we can use for analysis is
      calibration. For a group of markets to be perfectly calibrated, their
      average resolution values must match their average prediction values.
    </p>
    <p class="my-2">
      For example, let's say there are a handful of markets that will be
      determined by rolling a 6 on a fair six-sided die. We would expect each
      market to have an average probability of around 17%, and once they resolve
      we would expect around 17% of them to resolve positively. If both are
      true, then those markets were well-calibrated. If not, then some of our
      assumptions were incorrect.
    </p>
    <p class="my-2">
      This plot takes all of the prediction and resolution values and shows how
      closely they match. They should form a straight line from the bottom-left
      to the top-right - points significantly under or over that line represent
      systemic errors.
    </p>
    <div class="my-4">
      <CalibrationDefault platforms={platforms} markets={markets} />
    </div>
    <SeeMoreLink
      link={`/charts/calibration`}
      text="See all calibration charts"
    />
  </div>
</Base>
